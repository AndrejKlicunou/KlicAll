{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание на семинар 28 сентября по курсу анализа текстов ШАД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тема: Определение частей речи с помощью скрытой марковской модели и алгоритма Витерби. \n",
    "\n",
    "\n",
    "\n",
    "**Дедлайн**:   9:00 утра 5 октября (для заочников 9.00 утра 8 октября)\n",
    "\n",
    "**Среда выполнения**: Jupyter Notebook (Python 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "В данной  работе вам предстоит:\n",
    "\n",
    "- обучить скрытую марковскую модель на размеченных данных\n",
    "- реализовать алгоритм Витерби для декодирования\n",
    "\n",
    "Задание сдается в контест."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение частей речи (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем решать задачу определения частей речи (POS-теггинга) с помощью скрытой марковской модели (HMM). Формула совместной плотности наблюдаемых и скрытых переменных задается как\n",
    "\n",
    "$$ p(X, T) = p(T) p(X|T) = p(t_1)  \\prod_{i=2}^N p(t_i|t_{i-1}) \\prod_{i=1}^N p(x_i|t_i)$$\n",
    "\n",
    "В данном случае:\n",
    "\n",
    "- наблюдаемые переменные $X$ - это слова корпуса;\n",
    "\n",
    "- скрытые переменные $T$ - это POS-теги."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Обучение HMM на размеченных данных\n",
    "\n",
    "Требуется построить скрытую марковскую модель и настроить все ее параметры с помощью оценок максимального правдоподобия по размеченным данным (последовательности пар слово+тег):\n",
    "\n",
    "- Вероятности переходов между скрытыми состояниями $p(t_i | t_{i - 1})$ посчитайте на основе частот биграмм POS-тегов.\n",
    "\n",
    "- Вероятности эмиссий наблюдаемых состояний $p(x_i | t_i)$ посчитайте на основе частот \"POS-тег - слово\".\n",
    "\n",
    "- Обратите внимание на проблему разреженности счетчиков и сделаейте все вероятности сглаженными по Лапласу (add-one smoothing).\n",
    "\n",
    "- Распределение вероятностей начальных состояний $p(t_1)$ задайте равномерным.\n",
    "\n",
    "Обратите внимание, что так как мы используем размеченные данные, то у нас нет необходимости в оценивании апостериорных вероятностей на скрытые переменные с помощью алгоритма forward-backword и использовании EM-алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите brown корпус с универсальной системой тегирования. Для этого вам понадобятся ресурсы brown и universal_tagset из nltk.download().  В этой системе содержатся следующие теги:\n",
    "\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition\t(on, of, at, ...)\n",
    "- ADV - adverb\t(really, already, still, ...)\n",
    "- CONJ\t- conjunction\t(and, or, but, ...)\n",
    "- DET - determiner, article\t(the, a, some, ...)\n",
    "- NOUN\t- noun\t(year, home, costs, ...)\n",
    "- NUM - numeral\t(twenty-four, fourth, 1991, ...)\n",
    "- PRT -\tparticle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- .\t- punctuation marks\t(. , ;)\n",
    "- X\t- other\t(ersatz, esprit, dunno, ...)\n",
    "\n",
    "Тегсеты в корпусах текстов и в различных теггерах могут быть разными. Но есть маппер: http://www.nltk.org/_modules/nltk/tag/mapping.html\n",
    "\n",
    "Проанализируйте данные, с которыми Вы работаете. В частности, ответьте на вопросы:\n",
    "- Каков общий объем датасета, формат?\n",
    "- Приведены ли слова к нижнему регистру? Чем  это нам может в дальнейшем помешать?\n",
    "- Как распределены слова в корпусе?  Как распределены теги в корпусе?\n",
    "\n",
    "Сделайте случайное разбиение выборки на обучение и контроль в отношении 9:1 и обучите скрытую марковскую модель из предыдущего пункта. Если впоследствии обучение моделей будет занимать слишком много времени, работайте с подвыборкой, например, только текстами определенных категорий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_hmm(tagged_sents):\n",
    "    \"\"\"\n",
    "    Calucaltes p(tag), p(word|tag), p(tag|tag) from corpus.\n",
    "\n",
    "    Args:\n",
    "        tagged_sents: list of list of tagged tokens. \n",
    "            Example: \n",
    "            [[('dog', 'NOUN'), ('eats', 'VERB'), ...], ...]\n",
    "\n",
    "    Returns:\n",
    "        p_t, p_w_t, p_t_t - tuple of 3 elements:\n",
    "        p_t - dict(float), tag -> proba\n",
    "        p_w_t - dict(dict(float), tag -> word -> proba\n",
    "        p_t_t - dict(dict(float), previous_tag -> tag -> proba\n",
    "    \"\"\"\n",
    "    alpha=1e-24\n",
    "    counter_tag = Counter()\n",
    "    counter_tag_tag = Counter()\n",
    "    counter_tag_word = Counter()\n",
    "    tags = set()\n",
    "    words = set()\n",
    "    p_t_t = defaultdict(dict)\n",
    "    p_w_t = defaultdict(dict)\n",
    "    pt = dict()\n",
    "    for tagged_sent in tagged_sents:\n",
    "        i = 0\n",
    "        for tagged_word in tagged_sent:\n",
    "            words.add(tagged_word[0])\n",
    "            tags.add(tagged_word[1])\n",
    "            counter_tag_word[tuple(tagged_word)] += 1\n",
    "            counter_tag[tagged_word[1]] += 1\n",
    "            if i < len(tagged_sent) - 1:\n",
    "                counter_tag_tag[(tagged_sent[i][1], tagged_sent[i+1][1])] += 1\n",
    "            i += 1\n",
    "    for tag in tags:\n",
    "        p_t_t[tag] = defaultdict(float)\n",
    "        p_w_t[tag] = defaultdict(float)\n",
    "        pt[tag] = 1. / len(tags)\n",
    "    for (prev, cur), count in counter_tag_tag.items():\n",
    "        p_t_t[prev][cur] = (count + alpha) / (counter_tag[prev] + alpha * len(tags))\n",
    "    for (word, tag), count in counter_tag_word.items():\n",
    "        p_w_t[tag][word] = (count + alpha) / (counter_tag[tag] + alpha * (len(tags) + len(words)))\n",
    "    return pt, p_w_t, p_t_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "tagged_sents = list(brown_tagged_sents)\n",
    "random.shuffle(tagged_sents)\n",
    "train = tagged_sents[: int(0.9 * len(tagged_sents))]\n",
    "test = tagged_sents[int(0.9 * len(tagged_sents)) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt, p_w_t, p_t_t = train_hmm(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Алгоритм Витерби для применения модели\n",
    "\n",
    "Чтобы использовать обученную модель для определения частей речи на новых данных, необходимо реализовать алгоритм Витерби. Это алгоритм динамиеского программирования, с помощью которого мы будем находить наиболее вероятную последовательность скрытых состояний модели для фиксированной последовательности слов:\n",
    "\n",
    "$$ \\hat{T} = \\arg \\max_{T} p(T|X) = \\arg \\max_{T} p(X, T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, определяющую максимальную вероятность последовательности, заканчивающейся на $i$-ой позиции в состоянии $k$:\n",
    "\n",
    "$$\\delta(k, i) = \\max_{t_1, \\dots t_{i-1}} p(x_1, \\dots x_i, t_1, \\dots t_i=k)$$\n",
    "\n",
    "Тогда $\\max_{k} \\delta(k, N)$ - максимальная вероятность всей последовательности. А состояния, на которых эта вероятность достигается - ответ задачи.\n",
    "\n",
    "Алгоритм Витерби заключается в последовательном пересчете функции $\\delta(k, i)$ по формуле:\n",
    "\n",
    "$$\\delta(k, i) = \\max_{m} \\delta(m, i-1) p(t_i = k|t_{i-1} = m) p(x_i|t_i=k) $$\n",
    "\n",
    "Аналогично пересчитывается функция, определяющая, на каком состоянии этот максимум достигается:\n",
    "\n",
    "$$s(k, i) = \\arg \\max_{m} \\delta(m, i-1) p(t_i = k|t_{i-1} = m) p(x_i|t_i=k) $$\n",
    "\n",
    "\n",
    "На практике это означает заполнение двумерных массивов размерности: (длина последовательности) $\\times$ (количество возможных состояний). Когда массивы заполнены, $\\arg \\max_{k} \\delta(k, N)$ говорит о последнем состоянии. Начиная с него можно восстановить все состояния по массиву $s$. \n",
    "\n",
    "Осталось уточнить, как стартовать последовательный пересчет (чем заполнить первый столбец массива вероятностей):\n",
    "\n",
    "$$\\delta(k, 1) = p(k) p(x_1|t_1=k)$$\n",
    "\n",
    "В реализации рекомендуется перейти к логарифмам, т.к. произведение большого числа маленьких вероятностей может приводить к вычислительным ошибкам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(test_tokens_list, p_t, p_w_t, p_t_t):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        test_tokens_list: list of tokens. \n",
    "            Example: \n",
    "            ['I', 'go']\n",
    "        p_t: dict(float), tag->proba\n",
    "        p_w_t: - dict(dict(float), tag -> word -> proba\n",
    "        p_t_t: - dict(dict(float), previous_tag -> tag -> proba\n",
    "\n",
    "    Returns:\n",
    "        list of hidden tags\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    pos = list(p_t.keys())\n",
    "    s = np.zeros([len(p_t), len(test_tokens_list)], dtype=np.int)\n",
    "    delta = np.zeros([len(p_t), len(test_tokens_list)])\n",
    "    for i in range(len(p_t)):\n",
    "        delta[i][0] = np.log(max(10 ** (-100), p_t[pos[i]])) + \\\n",
    "                      np.log(max(10 ** (-100), p_w_t[pos[i]][test_tokens_list[0]]))\n",
    "    for i in range(1, len(test_tokens_list)):\n",
    "        for j in range(delta.shape[0]):\n",
    "            delta[j][i] = -10 ** 49\n",
    "            for k in range(delta.shape[0]):\n",
    "                cur_proba = delta[k][i-1] + np.log(max(p_t_t[pos[k]][pos[j]], 10 ** (-100))) + \\\n",
    "                            np.log(max(10 ** (-100), p_w_t[pos[j]][test_tokens_list[i]]))\n",
    "                if delta[j][i] < cur_proba:\n",
    "                    delta[j][i] = cur_proba\n",
    "                    s[j][i] = k\n",
    "    x = np.argmax(delta[:, delta.shape[1] - 1:])\n",
    "    result.append(pos[x])\n",
    "    for i in range(1, delta.shape[1]):\n",
    "        x = s[x][s.shape[1] - i]\n",
    "        result.append(pos[x])\n",
    "    return list(reversed(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте работу реализованного алгоритма на следующих модельных примерах, проинтерпретируйте результат.\n",
    "\n",
    "- 'he can stay'\n",
    "- 'a milk can'\n",
    "- 'i saw a dog'\n",
    "- 'an old saw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примените модель к отложенной выборке Брауновского корпуса и подсчитайте точность определения тегов (accuracy). Сделайте выводы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 0.9, 0.96, 0.9090909090909091, 0.9830508474576272, 1.0, 0.92, 1.0]\n"
     ]
    }
   ],
   "source": [
    "accur = []\n",
    "for j in range(100):\n",
    "    test_tokens_list = []\n",
    "    for word in test[j]:\n",
    "        test_tokens_list.append(word[0])\n",
    "    tags = viterbi_algorithm(test_tokens_list, pt, p_w_t, p_t_t)\n",
    "    correct_count = 0\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i] == test[j][i][1]:\n",
    "            correct_count += 1\n",
    "    accur.append(1. * correct_count / len(test[j]))\n",
    "print(accur[0: 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
