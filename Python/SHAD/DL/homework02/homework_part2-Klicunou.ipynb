{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 50% (50% points)\n",
    "    * 60% (60% points)\n",
    "    * 65% (70% points)\n",
    "    * 70% (80% points)\n",
    "    * 75% (90% points)\n",
    "    * 80% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
    "* you __can__ use validation data for training, but you __can't'__ do anything with test data apart from running the evaluation procedure.\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
    "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=path_to_cifar_like_in_seminar, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3, 32, 32) (40000,)\n"
     ]
    }
   ],
   "source": [
    "from cifar import load_cifar10\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_cifar10(\"cifar_data\")\n",
    "class_names = np.array(['airplane','automobile ','bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck'])\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=50, kernel_size=(3,3), padding=1),\n",
    "    nn.BatchNorm2d(50),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2,2)),\n",
    "    nn.Conv2d(in_channels=50, out_channels=100, kernel_size=(3,3), padding=2, dilation=2),\n",
    "    nn.BatchNorm2d(100),\n",
    "    nn.Dropout2d(0.4),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2,2)),\n",
    "    nn.Conv2d(in_channels=100, out_channels=150, stride = 2, kernel_size=(3,3), padding=1, dilation=1),\n",
    "    nn.BatchNorm2d(150),\n",
    "    nn.Dropout2d(0.3),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(16 * 150, 512),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    "    nn.BatchNorm1d(10),\n",
    "    nn.Softmax(dim=1)\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(torch.Tensor(X_train[:5]).cuda()).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1479816"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_number = 0\n",
    "for param in cnn.parameters():\n",
    "    i = 1\n",
    "    for item in param.size():\n",
    "        i *= item\n",
    "    param_number += i\n",
    "param_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch, model):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch)).cuda()\n",
    "    y_batch = Variable(torch.LongTensor(y_batch)).cuda()\n",
    "    logits = model(X_batch)\n",
    "    l2_loss = 0\n",
    "    param_number = 0\n",
    "    for param in model.parameters():\n",
    "        i = 1\n",
    "        for item in param.size():\n",
    "            i *= item\n",
    "        param_number += i\n",
    "        l2_loss += (param * param).sum()\n",
    "    return F.cross_entropy(logits, y_batch).mean() + l2_loss * 0.01 / param_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Training __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation([-30, 30]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "train_loader = CIFAR10(\"./cifar_data/\", train=True, transform=transform_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16f48c18>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_loader.train_data[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]\n",
    "\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_loader, \n",
    "                                                      batch_size=128,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=1)\n",
    "\n",
    "opt = torch.optim.Adam(cnn.parameters())\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time\n",
    "num_epochs = 100 \n",
    "batch_size = 128 \n",
    "sched1 = StepLR(opt, step_size=60, gamma = 0.1)\n",
    "sched2 = StepLR(opt, step_size=80, gamma = 0.1)\n",
    "sched3 = StepLR(opt, step_size=90, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 108.564s\n",
      "  training loss (in-iteration): \t1.957021\n",
      "  validation accuracy: \t\t\t57.66 %\n",
      "Epoch 2 of 100 took 107.572s\n",
      "  training loss (in-iteration): \t1.913710\n",
      "  validation accuracy: \t\t\t65.50 %\n",
      "Epoch 3 of 100 took 111.965s\n",
      "  training loss (in-iteration): \t1.888098\n",
      "  validation accuracy: \t\t\t66.55 %\n",
      "Epoch 4 of 100 took 110.559s\n",
      "  training loss (in-iteration): \t1.868989\n",
      "  validation accuracy: \t\t\t68.52 %\n",
      "Epoch 5 of 100 took 110.434s\n",
      "  training loss (in-iteration): \t1.856414\n",
      "  validation accuracy: \t\t\t66.83 %\n",
      "Epoch 6 of 100 took 112.146s\n",
      "  training loss (in-iteration): \t1.844265\n",
      "  validation accuracy: \t\t\t68.84 %\n",
      "Epoch 7 of 100 took 111.827s\n",
      "  training loss (in-iteration): \t1.838185\n",
      "  validation accuracy: \t\t\t66.94 %\n",
      "Epoch 8 of 100 took 109.577s\n",
      "  training loss (in-iteration): \t1.830069\n",
      "  validation accuracy: \t\t\t71.36 %\n",
      "Epoch 9 of 100 took 108.243s\n",
      "  training loss (in-iteration): \t1.823638\n",
      "  validation accuracy: \t\t\t72.23 %\n",
      "Epoch 10 of 100 took 110.431s\n",
      "  training loss (in-iteration): \t1.818647\n",
      "  validation accuracy: \t\t\t69.23 %\n",
      "Epoch 11 of 100 took 111.050s\n",
      "  training loss (in-iteration): \t1.813974\n",
      "  validation accuracy: \t\t\t73.41 %\n",
      "Epoch 12 of 100 took 109.959s\n",
      "  training loss (in-iteration): \t1.805417\n",
      "  validation accuracy: \t\t\t73.71 %\n",
      "Epoch 13 of 100 took 110.271s\n",
      "  training loss (in-iteration): \t1.800874\n",
      "  validation accuracy: \t\t\t74.61 %\n",
      "Epoch 14 of 100 took 109.590s\n",
      "  training loss (in-iteration): \t1.799513\n",
      "  validation accuracy: \t\t\t73.15 %\n",
      "Epoch 15 of 100 took 108.399s\n",
      "  training loss (in-iteration): \t1.794029\n",
      "  validation accuracy: \t\t\t73.72 %\n",
      "Epoch 16 of 100 took 111.923s\n",
      "  training loss (in-iteration): \t1.793946\n",
      "  validation accuracy: \t\t\t70.97 %\n",
      "Epoch 17 of 100 took 112.112s\n",
      "  training loss (in-iteration): \t1.787738\n",
      "  validation accuracy: \t\t\t71.29 %\n",
      "Epoch 18 of 100 took 108.634s\n",
      "  training loss (in-iteration): \t1.785273\n",
      "  validation accuracy: \t\t\t75.04 %\n",
      "Epoch 19 of 100 took 106.472s\n",
      "  training loss (in-iteration): \t1.784383\n",
      "  validation accuracy: \t\t\t74.74 %\n",
      "Epoch 20 of 100 took 106.407s\n",
      "  training loss (in-iteration): \t1.780023\n",
      "  validation accuracy: \t\t\t76.21 %\n",
      "Epoch 21 of 100 took 106.479s\n",
      "  training loss (in-iteration): \t1.777708\n",
      "  validation accuracy: \t\t\t73.55 %\n",
      "Epoch 22 of 100 took 106.238s\n",
      "  training loss (in-iteration): \t1.774196\n",
      "  validation accuracy: \t\t\t76.18 %\n",
      "Epoch 23 of 100 took 106.236s\n",
      "  training loss (in-iteration): \t1.775018\n",
      "  validation accuracy: \t\t\t75.26 %\n",
      "Epoch 24 of 100 took 106.853s\n",
      "  training loss (in-iteration): \t1.769960\n",
      "  validation accuracy: \t\t\t76.62 %\n",
      "Epoch 25 of 100 took 106.630s\n",
      "  training loss (in-iteration): \t1.767848\n",
      "  validation accuracy: \t\t\t75.77 %\n",
      "Epoch 26 of 100 took 106.683s\n",
      "  training loss (in-iteration): \t1.766417\n",
      "  validation accuracy: \t\t\t73.66 %\n",
      "Epoch 27 of 100 took 106.514s\n",
      "  training loss (in-iteration): \t1.763975\n",
      "  validation accuracy: \t\t\t77.20 %\n",
      "Epoch 28 of 100 took 106.928s\n",
      "  training loss (in-iteration): \t1.760810\n",
      "  validation accuracy: \t\t\t76.48 %\n",
      "Epoch 29 of 100 took 106.428s\n",
      "  training loss (in-iteration): \t1.759854\n",
      "  validation accuracy: \t\t\t76.57 %\n",
      "Epoch 30 of 100 took 106.499s\n",
      "  training loss (in-iteration): \t1.758396\n",
      "  validation accuracy: \t\t\t78.03 %\n",
      "Epoch 31 of 100 took 106.821s\n",
      "  training loss (in-iteration): \t1.757332\n",
      "  validation accuracy: \t\t\t76.59 %\n",
      "Epoch 32 of 100 took 106.333s\n",
      "  training loss (in-iteration): \t1.754903\n",
      "  validation accuracy: \t\t\t76.94 %\n",
      "Epoch 33 of 100 took 107.278s\n",
      "  training loss (in-iteration): \t1.755128\n",
      "  validation accuracy: \t\t\t77.29 %\n",
      "Epoch 34 of 100 took 105.983s\n",
      "  training loss (in-iteration): \t1.750318\n",
      "  validation accuracy: \t\t\t78.22 %\n",
      "Epoch 35 of 100 took 106.046s\n",
      "  training loss (in-iteration): \t1.748433\n",
      "  validation accuracy: \t\t\t78.64 %\n",
      "Epoch 36 of 100 took 105.971s\n",
      "  training loss (in-iteration): \t1.744955\n",
      "  validation accuracy: \t\t\t77.49 %\n",
      "Epoch 37 of 100 took 105.896s\n",
      "  training loss (in-iteration): \t1.746900\n",
      "  validation accuracy: \t\t\t78.51 %\n",
      "Epoch 38 of 100 took 106.342s\n",
      "  training loss (in-iteration): \t1.744097\n",
      "  validation accuracy: \t\t\t78.21 %\n",
      "Epoch 39 of 100 took 106.196s\n",
      "  training loss (in-iteration): \t1.745374\n",
      "  validation accuracy: \t\t\t78.30 %\n",
      "Epoch 40 of 100 took 105.888s\n",
      "  training loss (in-iteration): \t1.743086\n",
      "  validation accuracy: \t\t\t77.60 %\n",
      "Epoch 41 of 100 took 105.953s\n",
      "  training loss (in-iteration): \t1.740176\n",
      "  validation accuracy: \t\t\t74.44 %\n",
      "Epoch 42 of 100 took 105.935s\n",
      "  training loss (in-iteration): \t1.739183\n",
      "  validation accuracy: \t\t\t80.09 %\n",
      "Epoch 43 of 100 took 106.382s\n",
      "  training loss (in-iteration): \t1.739232\n",
      "  validation accuracy: \t\t\t77.28 %\n",
      "Epoch 44 of 100 took 106.317s\n",
      "  training loss (in-iteration): \t1.735615\n",
      "  validation accuracy: \t\t\t77.30 %\n",
      "Epoch 45 of 100 took 106.293s\n",
      "  training loss (in-iteration): \t1.738301\n",
      "  validation accuracy: \t\t\t78.07 %\n",
      "Epoch 46 of 100 took 106.047s\n",
      "  training loss (in-iteration): \t1.734555\n",
      "  validation accuracy: \t\t\t78.60 %\n",
      "Epoch 47 of 100 took 105.261s\n",
      "  training loss (in-iteration): \t1.733480\n",
      "  validation accuracy: \t\t\t78.49 %\n",
      "Epoch 48 of 100 took 104.907s\n",
      "  training loss (in-iteration): \t1.733151\n",
      "  validation accuracy: \t\t\t79.91 %\n",
      "Epoch 49 of 100 took 104.675s\n",
      "  training loss (in-iteration): \t1.732394\n",
      "  validation accuracy: \t\t\t79.19 %\n",
      "Epoch 50 of 100 took 105.441s\n",
      "  training loss (in-iteration): \t1.731911\n",
      "  validation accuracy: \t\t\t79.73 %\n",
      "Epoch 51 of 100 took 105.243s\n",
      "  training loss (in-iteration): \t1.730597\n",
      "  validation accuracy: \t\t\t80.54 %\n",
      "Epoch 52 of 100 took 104.720s\n",
      "  training loss (in-iteration): \t1.728647\n",
      "  validation accuracy: \t\t\t79.12 %\n",
      "Epoch 53 of 100 took 104.513s\n",
      "  training loss (in-iteration): \t1.727585\n",
      "  validation accuracy: \t\t\t80.41 %\n",
      "Epoch 54 of 100 took 104.762s\n",
      "  training loss (in-iteration): \t1.727083\n",
      "  validation accuracy: \t\t\t79.77 %\n",
      "Epoch 55 of 100 took 107.138s\n",
      "  training loss (in-iteration): \t1.726413\n",
      "  validation accuracy: \t\t\t79.79 %\n",
      "Epoch 56 of 100 took 104.258s\n",
      "  training loss (in-iteration): \t1.726276\n",
      "  validation accuracy: \t\t\t78.89 %\n",
      "Epoch 57 of 100 took 107.265s\n",
      "  training loss (in-iteration): \t1.726334\n",
      "  validation accuracy: \t\t\t79.80 %\n",
      "Epoch 58 of 100 took 108.726s\n",
      "  training loss (in-iteration): \t1.724836\n",
      "  validation accuracy: \t\t\t80.37 %\n",
      "Epoch 59 of 100 took 110.764s\n",
      "  training loss (in-iteration): \t1.721114\n",
      "  validation accuracy: \t\t\t79.83 %\n",
      "Epoch 60 of 100 took 110.505s\n",
      "  training loss (in-iteration): \t1.721228\n",
      "  validation accuracy: \t\t\t80.61 %\n",
      "Epoch 61 of 100 took 106.063s\n",
      "  training loss (in-iteration): \t1.720397\n",
      "  validation accuracy: \t\t\t79.97 %\n",
      "Epoch 62 of 100 took 105.550s\n",
      "  training loss (in-iteration): \t1.719126\n",
      "  validation accuracy: \t\t\t80.52 %\n",
      "Epoch 63 of 100 took 107.559s\n",
      "  training loss (in-iteration): \t1.718926\n",
      "  validation accuracy: \t\t\t80.25 %\n",
      "Epoch 64 of 100 took 107.901s\n",
      "  training loss (in-iteration): \t1.719637\n",
      "  validation accuracy: \t\t\t78.64 %\n",
      "Epoch 65 of 100 took 109.100s\n",
      "  training loss (in-iteration): \t1.719389\n",
      "  validation accuracy: \t\t\t80.25 %\n",
      "Epoch 66 of 100 took 111.617s\n",
      "  training loss (in-iteration): \t1.717539\n",
      "  validation accuracy: \t\t\t80.46 %\n",
      "Epoch 67 of 100 took 108.084s\n",
      "  training loss (in-iteration): \t1.714225\n",
      "  validation accuracy: \t\t\t79.71 %\n",
      "Epoch 68 of 100 took 107.623s\n",
      "  training loss (in-iteration): \t1.713849\n",
      "  validation accuracy: \t\t\t80.07 %\n",
      "Epoch 69 of 100 took 107.919s\n",
      "  training loss (in-iteration): \t1.715990\n",
      "  validation accuracy: \t\t\t81.05 %\n",
      "Epoch 70 of 100 took 108.253s\n",
      "  training loss (in-iteration): \t1.712090\n",
      "  validation accuracy: \t\t\t80.70 %\n",
      "Epoch 71 of 100 took 109.624s\n",
      "  training loss (in-iteration): \t1.713423\n",
      "  validation accuracy: \t\t\t80.99 %\n",
      "Epoch 72 of 100 took 108.479s\n",
      "  training loss (in-iteration): \t1.712478\n",
      "  validation accuracy: \t\t\t81.36 %\n",
      "Epoch 73 of 100 took 106.758s\n",
      "  training loss (in-iteration): \t1.710931\n",
      "  validation accuracy: \t\t\t79.63 %\n",
      "Epoch 74 of 100 took 107.390s\n",
      "  training loss (in-iteration): \t1.713201\n",
      "  validation accuracy: \t\t\t80.94 %\n",
      "Epoch 75 of 100 took 106.872s\n",
      "  training loss (in-iteration): \t1.710402\n",
      "  validation accuracy: \t\t\t79.95 %\n",
      "Epoch 76 of 100 took 106.918s\n",
      "  training loss (in-iteration): \t1.709749\n",
      "  validation accuracy: \t\t\t81.17 %\n",
      "Epoch 77 of 100 took 107.688s\n",
      "  training loss (in-iteration): \t1.709082\n",
      "  validation accuracy: \t\t\t81.42 %\n",
      "Epoch 78 of 100 took 108.594s\n",
      "  training loss (in-iteration): \t1.710065\n",
      "  validation accuracy: \t\t\t81.41 %\n",
      "Epoch 79 of 100 took 107.872s\n",
      "  training loss (in-iteration): \t1.707645\n",
      "  validation accuracy: \t\t\t80.04 %\n",
      "Epoch 80 of 100 took 106.720s\n",
      "  training loss (in-iteration): \t1.708198\n",
      "  validation accuracy: \t\t\t80.91 %\n",
      "Epoch 81 of 100 took 107.496s\n",
      "  training loss (in-iteration): \t1.708346\n",
      "  validation accuracy: \t\t\t81.91 %\n",
      "Epoch 82 of 100 took 106.839s\n",
      "  training loss (in-iteration): \t1.704502\n",
      "  validation accuracy: \t\t\t81.04 %\n",
      "Epoch 83 of 100 took 106.177s\n",
      "  training loss (in-iteration): \t1.704637\n",
      "  validation accuracy: \t\t\t80.29 %\n",
      "Epoch 84 of 100 took 106.019s\n",
      "  training loss (in-iteration): \t1.703734\n",
      "  validation accuracy: \t\t\t81.76 %\n",
      "Epoch 85 of 100 took 106.339s\n",
      "  training loss (in-iteration): \t1.705503\n",
      "  validation accuracy: \t\t\t80.37 %\n",
      "Epoch 86 of 100 took 106.140s\n",
      "  training loss (in-iteration): \t1.706667\n",
      "  validation accuracy: \t\t\t81.75 %\n",
      "Epoch 87 of 100 took 106.499s\n",
      "  training loss (in-iteration): \t1.704483\n",
      "  validation accuracy: \t\t\t80.96 %\n",
      "Epoch 88 of 100 took 106.329s\n",
      "  training loss (in-iteration): \t1.699043\n",
      "  validation accuracy: \t\t\t82.47 %\n",
      "Epoch 89 of 100 took 108.386s\n",
      "  training loss (in-iteration): \t1.693512\n",
      "  validation accuracy: \t\t\t82.59 %\n",
      "Epoch 90 of 100 took 109.059s\n",
      "  training loss (in-iteration): \t1.691770\n",
      "  validation accuracy: \t\t\t82.68 %\n",
      "Epoch 91 of 100 took 108.143s\n",
      "  training loss (in-iteration): \t1.688963\n",
      "  validation accuracy: \t\t\t82.58 %\n",
      "Epoch 92 of 100 took 106.510s\n",
      "  training loss (in-iteration): \t1.688505\n",
      "  validation accuracy: \t\t\t82.86 %\n",
      "Epoch 93 of 100 took 108.924s\n",
      "  training loss (in-iteration): \t1.684964\n",
      "  validation accuracy: \t\t\t82.60 %\n",
      "Epoch 94 of 100 took 108.852s\n",
      "  training loss (in-iteration): \t1.685833\n",
      "  validation accuracy: \t\t\t83.31 %\n",
      "Epoch 95 of 100 took 110.432s\n",
      "  training loss (in-iteration): \t1.685938\n",
      "  validation accuracy: \t\t\t82.82 %\n",
      "Epoch 96 of 100 took 110.063s\n",
      "  training loss (in-iteration): \t1.684740\n",
      "  validation accuracy: \t\t\t82.88 %\n",
      "Epoch 97 of 100 took 109.217s\n",
      "  training loss (in-iteration): \t1.681874\n",
      "  validation accuracy: \t\t\t83.31 %\n",
      "Epoch 98 of 100 took 109.454s\n",
      "  training loss (in-iteration): \t1.683350\n",
      "  validation accuracy: \t\t\t83.30 %\n",
      "Epoch 99 of 100 took 109.517s\n",
      "  training loss (in-iteration): \t1.682514\n",
      "  validation accuracy: \t\t\t83.01 %\n",
      "Epoch 100 of 100 took 110.506s\n",
      "  training loss (in-iteration): \t1.682896\n",
      "  validation accuracy: \t\t\t83.03 %\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        sched1.step()\n",
    "        sched2.step()\n",
    "        sched3.step()\n",
    "        \n",
    "        cnn.train(True) \n",
    "        for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "            loss = compute_loss(X_batch, y_batch, cnn)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_loss.append(loss.data.cpu().numpy())\n",
    "   \n",
    "        for (x_batch, y_batch) in train_batch_gen:\n",
    "            loss = compute_loss(x_batch, y_batch, cnn)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_loss.append(loss.data.cpu().numpy())\n",
    "            \n",
    "        cnn.train(False)\n",
    "        for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "            logits = cnn(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "            y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "            val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss[-len(X_train) // batch_size - train_loader.train_data.shape[0] // batch_size :])))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "            np.mean(val_accuracy[-len(X_val) // batch_size :]) * 100))\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t81.90 %\n",
      "Achievement unlocked: 110lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "cnn.train(False)\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    logits = cnn(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отчёт.\n",
    "\n",
    "Сначала я взял сеть поменьше, чтобы на ней потестировать все \"плюшки\", которые предлагались в качестве улучшающих результат. В ней было 2 свёрточных и 2 полносвязных слоя, при этом агресcивно уменьшался размер входа путём использования stride и max pooling. \n",
    "\n",
    "#### 1) Батчнормы и дропауты. \n",
    "Наставил везде, где только можно. Качество улучшалось, сеть училась.\n",
    "\n",
    "#### 2) Аугментация.\n",
    "Я человек простой: вижу код - беру. Убрал только нормализацию оттуда. Потыкал в индексы, посмотрел на картинки, вроде преобразования их не сильно испортили. Учиться стало дольше, но результат повысился.\n",
    "\n",
    "#### 3) Регуляризация функции потерь.\n",
    "Хотел брал ту штуку, что по ссылке шла, но с ней пришлось повозиться, а ещё она не очень хорошо работала. Я, когда тестировал, добавил ещё l1-регуляризацию, возможно, она всё поломала. Тут я забыл, что человек простой, и написал свою реализацию, где руками доставал параметры и применял модуль / возведение в квадрат. Потом считал среднее по всем параметрам и умножал на 0.01. Без l1-регуляризации (код вытер) стало лучше. Игрался с весом (брал 0.1, 0.01, 1), мне больше всего понравился вариант с 0.01, т.к. обучение менее штормило, а регуляризатор есть.\n",
    "\n",
    "#### 4) Изменение шага обучения.\n",
    "Нашёл самый удобный вариант реализации и вставил, сначала было только 2 уменьшения learning rate, step_size=45. С той эпохи, как lr уменьшался, наблюдалось более быстрое уменьшение функции потерь на обучающей выборке. Для финальной версии я решил, что стоит сделать несколько таких \"встрясок\", но при этом на стандартном значении параметра я давал 60 эпох с аугментацией, это меня обнадёживало в смысле того, что вдруг я рано замедлил темп обучения. Оставил в итоге тот вариант, который есть (уменьшаем на 60-й, 80-й и 90-й эпохах).\n",
    "\n",
    "#### 5) Итоговая сеть.\n",
    "Ограничился 3 свёрточными и 3 линейными слоями. Сначала было 2 линейных, но потом добавил ещё один, и стало лучше. Идея свёрточных слоёв была в том, чтобы по размерностям уменшить картинку в 8 раз, а receptive field был побольше (поэтому вторая свёртка с dliation=2), при этом выход последнего свёрточного слоя хотелось оставить, пэотому только 2 пулинга, но добавил stride=2. \n",
    "\n",
    "Сначала пробовал сетки с меньшим числом нейронов, но 80% качества на тестовом множестве не достигал. Пытался понемногу добавлять число каналов в свёрточных слоях, но не хватало немного до 80%. В некоторый момент мне это надоело, и я сделал ЕЩЁ БОЛЬШЕ НЕЙРОНОВ НУ ЧТОБЫ УЖЕ ТОЧНО ПОЛУЧИТЬ ACCURACY 80%, одолжил мощный комп и получил требуемое качество - 81.90% на тестовом множестве и 83.03% на валидационном."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, my name is `___ ___`, and here's my story\n",
    "\n",
    "A long time ago in a galaxy far far away, when it was still more than an hour before the deadline, i got an idea:\n",
    "\n",
    "##### I gonna build a neural network, that\n",
    "* brief text on what was\n",
    "* the original idea\n",
    "* and why it was so\n",
    "\n",
    "How could i be so naive?!\n",
    "\n",
    "##### One day, with no signs of warning,\n",
    "This thing has finally converged and\n",
    "* Some explaination about what were the results,\n",
    "* what worked and what didn't\n",
    "* most importantly - what next steps were taken, if any\n",
    "* and what were their respective outcomes\n",
    "\n",
    "##### Finally, after __  iterations, __ mugs of [tea/coffee]\n",
    "* what was the final architecture\n",
    "* as well as training method and tricks\n",
    "\n",
    "That, having wasted ____ [minutes, hours or days] of my life training, got\n",
    "\n",
    "* accuracy on training: __\n",
    "* accuracy on validation: __\n",
    "* accuracy on test: __\n",
    "\n",
    "\n",
    "[an optional afterword and mortal curses on assignment authors]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
