{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Лабораторная работа 3. Линейные методы.\n",
    "\n",
    "Результат лабораторной работы − отчет. Мы предпочитаем принимать отчеты в формате ноутбуков IPython (ipynb-файл). Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий. Помимо ответов на вопросы, в отчете также должен быть код, однако чем меньше кода, тем лучше всем: нам − меньше проверять, вам — проще найти ошибку или дополнить эксперимент. При проверке оценивается четкость ответов на вопросы, аккуратность отчета и кода.\n",
    "\n",
    "Мы уверены, что выполнение лабораторных работ занимает значительное время, поэтому не рекомендуем оставлять их на последний вечер перед сдачей.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 15 баллов. Сдавать задание после указанного срока сдачи нельзя. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов и понижают карму (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий в открытом источнике, необходимо прислать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, нам необходима ссылка на источник).\n",
    "\n",
    "Обратите внимание, что мы не ставим оценку за просто написанный код, корректная работоспособность которого не подтверждена экспериментами.\n",
    "\n",
    "### Правила сдачи\n",
    "Выполненную работу следует отправить в систему Anytask. Более подробно о системе можно почитать на странице курса. Название отправляемого файла должно иметь следующий формат: Surname_Name_Group_NN.ipynb, где NN — номер лабораторной работы. Например, Kozlova_Anna_CS_03.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='log_reg'></a>\n",
    "## Логистическая регрессия\n",
    "\n",
    "В этом пункте мы будем рассматривать бинарную классификацию, где метки классов лежат во множестве $\\{-1, 1\\}$. \n",
    "\n",
    "Задачу обучения регуляризованной логистической регрессии можно записать следующим образом:\n",
    "\n",
    "$$ \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + \\exp(-\\langle w, x_i \\rangle y_i)) + \\dfrac{C}{2}\\lVert w \\rVert^2  \\to \\min_w$$\n",
    "\n",
    "Обучение в данном случае сводится к нахождению параметров модели $w$, которое производится с помощью метода градиентного спуска (Gradient Descent, GD). \n",
    "\n",
    "Градиентный шаг будет заключаться в обновлении вектора весов по следующей формуле:\n",
    "\n",
    "$$w := w + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w, x_i \\rangle y_i)}\\Big) - \\eta Cw$$\n",
    "\n",
    "где $\\eta > 0$ — размер шага (learning rate).\n",
    "\n",
    "В общем случае метод градиентного спуска имеет следующие недостатки:\n",
    "- попадание в локальные минимумы\n",
    "- неочевидность критерия останова\n",
    "- выбор размера шага\n",
    "- начальная инициализация весов\n",
    "\n",
    "В этой части лабораторной работы мы предложим вам реализовать метод градиентного спуска, а также рассмотрим некоторые его модификации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте выборку из 1000 точек с 20 признаками на которой будете проводить эксперименты. Мы рекомендуем воспользоваться функцией [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) из пакета sklearn. Обратите внимание, что метки классов для данной задачи должны быть из множества {-1, 1} (по умолчанию make_classification возвращает метки из множества {0, 1})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3.5 балла)** Реализуйте градиентный спуск и протестируйте его для случая логистической регрессии на ранее сгенерированной выборке. Для сравнения качества разных подходов используйте значение оптимизируемого функционала.\n",
    "\n",
    "В качестве критерия останова мы предлагаем использовать следующие условия:\n",
    " - евклидова норма разности текущего и нового векторов весов стала меньше, чем 1e-6\n",
    " - ограничение на число итераций (например, 10000)\n",
    " \n",
    "Для начальной инициализации весов нужно сравнить следующие подходы:\n",
    " - нулевая начальная инициализация\n",
    " - случайная\n",
    " \n",
    "Выполните следующие пункты и прокомментируйте полученные результаты:\n",
    "- Рассмотрите как влияет размер шага на сходимость.\n",
    "- Рассмотрите регуляризованную модель, которая описана выше, а также модель без регуляризатора. Сравните влияет ли наличие регуляризации на скорость сходимости и качество.\n",
    "- Постройте график качества оптимизируемого функционала в зависимости от номера итерации (при правильной реализации и подходящем размере шага он должен убывать).\n",
    "- Влияет ли выбор начальной инициализации весов на скорость и качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "x, y = sklearn.datasets.make_classification(1000, 20)\n",
    "y[y == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(size, init_type = 'random'):\n",
    "    if (init_type == 'random'):\n",
    "        return np.random.uniform(-1./(2*size), 1./(2*size), size)\n",
    "    elif (init_type == 'zero'):\n",
    "        return np.zeros(size)\n",
    "\n",
    "def grad_descent(x, y, eta, C=0, iters_limit = 10000, w_init = 'random'):\n",
    "    N = x.shape[0]\n",
    "    n = x.shape[1]\n",
    "    all_w_iterations = []\n",
    "    w = init_weights(n, w_init)\n",
    "    all_w_iterations.append(w)\n",
    "    w_old = w\n",
    "    temp = np.zeros(n)\n",
    "    for i in range(N):\n",
    "        temp += x[i]*y[i]*(1-1./(1 + np.exp(-y[i]*x[i].dot(w_old.T))))\n",
    "    w = w_old + (eta*1./N)*temp - C*eta*w_old\n",
    "    all_w_iterations.append(w)\n",
    "    iters_count = 1\n",
    "    while np.linalg.norm(w-w_old) > 1e-6 and iters_count < iters_limit:\n",
    "        w_old = w\n",
    "        temp = np.zeros(n)\n",
    "        for i in range(N):\n",
    "            temp += x[i]*y[i]*(1-1./(1 + np.exp(-y[i]*(x[i].dot(w_old.T)))))\n",
    "        w = w_old + (eta*1./x.shape[0])*temp - C*eta*w_old\n",
    "        all_w_iterations.append(w)\n",
    "        iters_count += 1\n",
    "    return w, iters_count, all_w_iterations\n",
    "    \n",
    "def count_functional(x, y, w, C=0):\n",
    "    N = x.shape[0]\n",
    "    result = 0\n",
    "    for i in range(N):\n",
    "        result += np.log(1 + np.exp(-y[i]*x[i].dot(w.T)))/N\n",
    "    result += (C/2)*(np.linalg.norm(w))**2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим, как зависит число итераций от значения параметра \"эта\", а также посмотрим на значение функционала (начальная инициализация - случайный вектор, координаты которого равномерно распределены на $[-\\frac{1}{2n}, \\frac{1}{2n}]$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations with eta=0.01:  590\n",
      "Value of functional with this eta:  0.556672212428\n",
      "Number of iterations with eta=0.02:  324\n",
      "Value of functional with this eta:  0.556672209692\n",
      "Number of iterations with eta=0.05:  142\n",
      "Value of functional with this eta:  0.556672208812\n",
      "Number of iterations with eta=0.1:  74\n",
      "Value of functional with this eta:  0.556672208683\n",
      "Number of iterations with eta=0.3:  25\n",
      "Value of functional with this eta:  0.556672208659\n",
      "Number of iterations with eta=0.5:  14\n",
      "Value of functional with this eta:  0.556672208658\n",
      "Number of iterations with eta=0.75:  17\n",
      "Value of functional with this eta:  0.556672208657\n",
      "Number of iterations with eta=1:  148\n",
      "Value of functional with this eta:  0.556672208658\n"
     ]
    }
   ],
   "source": [
    "w, iters, w_s = grad_descent(x, y, 0.01, C=1)\n",
    "print('Number of iterations with eta=0.01: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.02, C=1)\n",
    "print('Number of iterations with eta=0.02: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.05, C=1)\n",
    "print('Number of iterations with eta=0.05: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.1, C=1)\n",
    "print('Number of iterations with eta=0.1: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.3, C=1)\n",
    "print('Number of iterations with eta=0.3: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.5, C=1)\n",
    "print('Number of iterations with eta=0.5: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.75, C=1)\n",
    "print('Number of iterations with eta=0.75: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 1, C=1)\n",
    "print('Number of iterations with eta=1: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нетрудно видеть, что с увеличением значения параметра \"эта\" количество итераций метода уменьшается, то есть метод сходится быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Расмотрим нерегуляризованную модель и сравним её с регуляризованной по значению минимизируемого функционала.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations with eta=0.01:  10000\n",
      "Value of functional with this eta:  0.171234180255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-79b7b0b450fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Value of functional with this eta: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_functional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of iterations with eta=0.02: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Value of functional with this eta: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_functional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-203a091adaba>\u001b[0m in \u001b[0;36mgrad_descent\u001b[0;34m(x, y, eta, C, iters_limit, w_init)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtemp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_old\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mall_w_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w, iters, w_s = grad_descent(x, y, 0.01)\n",
    "print('Number of iterations with eta=0.01: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.02)\n",
    "print('Number of iterations with eta=0.02: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что сходится метод очень медленно, при этом даёт меньшее значение минимизируемого функционала. Значит, присутствие регуляризатора ускоряет сходимость метода, но при этом значение функционала ухудшается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Построим график зависимости качества функционала от номера итерации.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efc7924bf28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAEWCAYAAAB7W6PxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVVp89vEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc7b6f36a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, iters, w_s = grad_descent(x, y, 0.02, C=1)\n",
    "funcs = []\n",
    "for i in w_s:\n",
    "    funcs.append(count_functional(x, y, i, C=1))\n",
    "plt.plot(np.arange(iters+1), funcs, color = 'darkblue', label = 'functional(iteration_number)')\n",
    "plt.title('value of functional from iteration number')\n",
    "plt.xlabel('iteration number')\n",
    "plt.ylabel('value of functional')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На графике значение функционала убывает с увеличением количества итераций, как и ожидалось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рассмотрим теперь метод, где первоначально вектор весов инициализируем нулевыми значениями и посмотрим, как будут результаты отличаться от метода со случайной первоначальной инициализацией.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations with eta=0.01:  564\n",
      "Value of functional with this eta:  0.55667221199\n",
      "Number of iterations with eta=0.02:  304\n",
      "Value of functional with this eta:  0.556672209483\n",
      "Number of iterations with eta=0.05:  132\n",
      "Value of functional with this eta:  0.556672208779\n",
      "Number of iterations with eta=0.1:  69\n",
      "Value of functional with this eta:  0.556672208681\n",
      "Number of iterations with eta=0.3:  22\n",
      "Value of functional with this eta:  0.556672208659\n"
     ]
    }
   ],
   "source": [
    "w, iters, w_s = grad_descent(x, y, 0.01, C=1, w_init='zero')\n",
    "print('Number of iterations with eta=0.01: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.02, C=1, w_init='zero')\n",
    "print('Number of iterations with eta=0.02: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.05, C=1, w_init='zero')\n",
    "print('Number of iterations with eta=0.05: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.1, C=1, w_init='zero')\n",
    "print('Number of iterations with eta=0.1: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x, y, 0.3, C=1, w_init='zero')\n",
    "print('Number of iterations with eta=0.3: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w, C=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате метод с нулевой начальной реализацией даёт примерно то же значение функционала, что и метод со случайной начальной инициализацией, а количество итераций меньше, но совсем ненамного (где-то пара-тройка процентов от общего числа итераций, а то и меньше)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска может быть весьма трудозатратен в случае большого размера обучающей выборки. Поэтому обычно используют метод стохастического градиентного спуска, где на каждой итерации выбирается случайный объект из обучающей выборки и обновление весов происходит сразу по этому объекту. \n",
    "\n",
    "**(1 балл)** Реализуйте метод стохастического градиентного спуска (sgd). В этом случае вы можете выбрать наиболее удачный функционал, исходя из предыдущего пункта (с регуляризацией, без), а также схему начальной инициализации весов.\n",
    "\n",
    "- Посмотрите как влияет размер шага на сходимость\n",
    "- Постройте график качества оптимизируемого функционала в зависимости от номера итерации \n",
    "\n",
    "Сравните рассмотренные методы (градиентный спуск и sgd) между собой с точки зрения скорости сходимости и качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Будем реализовывать метод без регуляризации, по умолчанию первоначальная генерация весов случайна, а коэффициент темпа забывания равен 0.5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L(x, y, w):\n",
    "    return np.log(1 + np.exp(-y*x.dot(w.T)))\n",
    "\n",
    "def nabla_L(x, y, w):\n",
    "    return x*y*(1-1./(1 + np.exp(-y*(x.dot(w.T)))))\n",
    "    \n",
    "def stoch_grad_descent(x, y, h = 0.5, lambda_ = 0.5, w_init = 'random'):\n",
    "    N = x.shape[0]\n",
    "    n = x.shape[1]\n",
    "    all_w_iterations = []\n",
    "    w = init_weights(n, w_init)\n",
    "    all_w_iterations.append(w)\n",
    "    Q = count_functional(x, y, w)\n",
    "    w_old = w\n",
    "    Q_old = Q\n",
    "    index = np.random.randint(0, N, 1)\n",
    "    eps = L(x[index[0]], y[index[0]], w_old)\n",
    "    w = w_old + h*nabla_L(x[index[0]], y[index[0]], w_old)\n",
    "    all_w_iterations.append(w)\n",
    "    Q = lambda_*eps + (1-lambda_)*Q_old\n",
    "    iters_count = 1\n",
    "    while np.linalg.norm(w-w_old) > 1e-6 and np.absolute(Q-Q_old) > 1e-6:\n",
    "        w_old = w\n",
    "        Q_old = Q\n",
    "        index = np.random.randint(0, N, 1)\n",
    "        eps = L(x[index[0]], y[index[0]], w_old)\n",
    "        w = w_old + h*nabla_L(x[index[0]], y[index[0]], w_old)\n",
    "        all_w_iterations.append(w)\n",
    "        Q = lambda_*eps + (1-lambda_)*Q_old\n",
    "        iters_count += 1\n",
    "    return w, iters_count, all_w_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на количество итераций и значение функционала для построенного метода.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations with eta=0.01:  5733\n",
      "Value of functional with this eta:  0.262044763722\n",
      "Number of iterations with eta=0.02:  1352\n",
      "Value of functional with this eta:  0.268669954373\n",
      "Number of iterations with eta=0.05:  1595\n",
      "Value of functional with this eta:  0.268200913807\n",
      "Number of iterations with eta=0.1:  614\n",
      "Value of functional with this eta:  0.338200585014\n",
      "Number of iterations with eta=0.3:  81\n",
      "Value of functional with this eta:  0.485277292197\n",
      "Number of iterations with eta=0.5:  35\n",
      "Value of functional with this eta:  0.504220209538\n",
      "Number of iterations with eta=0.75:  43\n",
      "Value of functional with this eta:  1.40702805712\n"
     ]
    }
   ],
   "source": [
    "w, iters, w_s = stoch_grad_descent(x, y, 0.01)\n",
    "print('Number of iterations with eta=0.01: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x, y, 0.02)\n",
    "print('Number of iterations with eta=0.02: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x, y, 0.05)\n",
    "print('Number of iterations with eta=0.05: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x, y, 0.1)\n",
    "print('Number of iterations with eta=0.1: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x, y, 0.3)\n",
    "print('Number of iterations with eta=0.3: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x, y, 0.5)\n",
    "print('Number of iterations with eta=0.5: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x, y, 0.75)\n",
    "print('Number of iterations with eta=0.75: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение шага обучения уменьшает количество итераций, но при этом значение минимизируемого функционала может пострадать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Построим график зависимости качества функционала от номера итерации.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efc791451d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAEWCAYAAACDu1gGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc7922bb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, iters, w_s = stoch_grad_descent(x, y, 0.02)\n",
    "funcs = []\n",
    "for i in w_s:\n",
    "    funcs.append(count_functional(x, y, i))\n",
    "plt.plot(np.arange(iters+1), funcs, color = 'darkblue', label = 'functional(iteration_number)')\n",
    "plt.title('value of functional from iteration number')\n",
    "plt.xlabel('iteration number')\n",
    "plt.ylabel('value of functional')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, график - убывающая функция, что говорит о том, что минимизируемый функционал уменьшается на каждой следующей итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод стохастического градиентного спуска лучше минимизиреут функционал, чем регуляризованный нестохастический градиентный спуск, но по результатам нерегуляризованного нестохастического градиентного спуска (хоть тот и плохо сходится) можно говорить, что функционал минимизируется ненамного лучше стохастическим методом, чем нестохастическим. Количество итераций, очевидно, куда больше у стохастического метода, но это большое количество выполняется во много раз быстрее, чем большое количество итераций нестохастического метода. В плане реализации второй метод ненамного сложнее первого.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Добавьте в выборку случайный шум следующим способом:\n",
    " 1. Выберите случайно 3% объектов из сгенерированной ранее выборки.\n",
    " 2. Сдвиньте эти объекты на $\\mathcal{N}(0; 3 * scale)$, где $scale$ —  масштаб, который был использован при генерации объектов (параметр $scale$ в make_classification).\n",
    " 3. Инвертируйте классы выбранных объектов.\n",
    " \n",
    "Для GD и SGD сравните скорость сходимости и значение функции потерь до и после добавления шума. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.random.randint(0, x.shape[0], int(x.shape[0]*0.03))\n",
    "x_ = np.empty_like(x)\n",
    "x_[:] = x\n",
    "y_ = np.empty_like(y)\n",
    "y_[:] = y\n",
    "x_[indices] += np.random.normal(0, 3., 1)[0]\n",
    "y_[indices] = y_[indices]*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GD:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations with eta=0.01:  644\n",
      "Value of functional with this eta:  0.610946925338\n",
      "Number of iterations with eta=0.02:  340\n",
      "Value of functional with this eta:  0.61094692221\n",
      "Number of iterations with eta=0.05:  149\n",
      "Value of functional with this eta:  0.61094692147\n",
      "Number of iterations with eta=0.1:  76\n",
      "Value of functional with this eta:  0.610946921367\n",
      "Number of iterations with eta=0.3:  27\n",
      "Value of functional with this eta:  0.61094692134\n"
     ]
    }
   ],
   "source": [
    "w, iters, w_s = grad_descent(x_, y_, 0.01, C=1)\n",
    "print('Number of iterations with eta=0.01: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x_, y_, 0.02, C=1)\n",
    "print('Number of iterations with eta=0.02: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x_, y_, 0.05, C=1)\n",
    "print('Number of iterations with eta=0.05: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x_, y_, 0.1, C=1)\n",
    "print('Number of iterations with eta=0.1: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w, C=1))\n",
    "\n",
    "w, iters, w_s = grad_descent(x_, y_, 0.3, C=1)\n",
    "print('Number of iterations with eta=0.3: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w, C=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значение функционала немного увеличилось, количество итераций уменьшилось, но ненамного.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCD:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations with eta=0.01:  13372\n",
      "Value of functional with this eta:  0.286926394516\n",
      "Number of iterations with eta=0.02:  22965\n",
      "Value of functional with this eta:  0.270921075147\n",
      "Number of iterations with eta=0.05:  21545\n",
      "Value of functional with this eta:  0.308937719891\n",
      "Number of iterations with eta=0.1:  1317\n",
      "Value of functional with this eta:  0.341101006595\n",
      "Number of iterations with eta=0.3:  118\n",
      "Value of functional with this eta:  0.444700109129\n"
     ]
    }
   ],
   "source": [
    "w, iters, w_s = stoch_grad_descent(x_, y_, 0.01)\n",
    "print('Number of iterations with eta=0.01: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x_, y_, 0.02)\n",
    "print('Number of iterations with eta=0.02: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x_, y_, 0.05)\n",
    "print('Number of iterations with eta=0.05: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x_, y_, 0.1)\n",
    "print('Number of iterations with eta=0.1: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w))\n",
    "\n",
    "w, iters, w_s = stoch_grad_descent(x_, y_, 0.3)\n",
    "print('Number of iterations with eta=0.3: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x_, y_, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В случае этого метода шум значительно увеличил для маленького шага обучения количество итераций, значение функционала увеличилось.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Между обновлением вектора весов по всей выборке и на одном объекте есть промежуточный подход — выбирать некоторое случайное подмножество объектов и обновлять веса по нему. Такой подход называется mini-batch. Мы не будем реализовывать этот подход в данной работе, однако иногда его бывает осмысленно использовать на практике. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из недостатков sgd состоит в том, что он может не доходить до локального оптимального решения, а осциллировать в окрестности. \n",
    "\n",
    "![](http://sebastianruder.com/content/images/2015/12/without_momentum.gif)\n",
    "\n",
    "Для решения этой проблемы существуют методы, позволяющие устранить этот недостаток, а также ускорить сходимость. Рассмотрим некоторые из них.\n",
    "\n",
    "![](http://nghenglim.github.io/images/2015061300.png)\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ v_t = \\gamma v_{t - 1} + \\eta \\nabla_{\\theta}{J(\\theta)}$$\n",
    "$$ \\theta = \\theta - v_t$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\theta$ — вектор параметров (в нашем случае — $w$)\n",
    " - $J$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n",
    " \n",
    "### Adagrad \n",
    "\n",
    "Одной из сложностей является выбор размера шага (*learning rate*). Основное отличие данного метода от SGD состоит в том что размер шага определяется для каждого параметра индивидуально. Этот метод хорошо работает с разреженным данным большого объема. \n",
    "\n",
    "Обозначим градиент по параметру $\\theta_i$ на итерации $t$ как $g_{t,i} = \\nabla_{\\theta}J(\\theta_i)$. \n",
    "\n",
    "В случае sgd обновление параметра $\\theta_i$ будет выглядеть следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t,i}$$\n",
    "\n",
    "А в случае Adagrad общий шаг $\\eta$ нормируется на посчитанные ранее градиенты для данного параметра:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t,ii} + \\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "где $G_t$ — диагональная матрица, где каждый диагональный элемент $i,i$ — сумма квадратов градиентов для $\\theta_{i}$ до $t$-ой итерации. $\\varepsilon$ — гиперпараметр, позволяющий избежать деления на 0 (обычно выбирается около *1e-8*).\n",
    "\n",
    "Так как матрица $G_t$ диагональна, в векторном виде это будет выглядеть следующим образом (здесь $\\odot$ — матричное умножение):\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\odot g_t $$\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "Adadelta, в отличии от Adagrad, рассматривает не все предыдущие значения градиентов, а только последние $k$. Кроме того, сумма градиентов определяется как уменьшающееся среднеее всех предыдущих квадратов градиентов. Текущее среднее $E[g^2]_t$ на итерации $t$ будет вглядеть следующим образом:\n",
    "\n",
    "$$ E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2 $$\n",
    "\n",
    "здесь $\\gamma$ аналогична гиперпараметру из метода Momentum.\n",
    "\n",
    "Тогда обновление весов можно записать следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Перепишем это немного по-другому:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\Delta \\theta_t$$ \n",
    "$$\\Delta \\theta_t = - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Аналогично среднему для градиентов определим среднее для параметров $\\theta$:\n",
    "\n",
    "$$ E[\\Delta \\theta^2]_t = \\gamma E[\\Delta \\theta^2]_{t-1} + (1-\\gamma)\\Delta \\theta^2 $$\n",
    "\n",
    "Введем обозначение $RMS[p]_t = \\sqrt{E[p]_t + \\varepsilon}$\n",
    "\n",
    "Тогда Adadelta выглядит следующим образом:\n",
    "\n",
    "$$\\Delta \\theta_t = - \\dfrac{RMS[\\Delta \\theta^2]}{RMS[ga^2]} g_t $$ \n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\Delta \\theta_t$$ \n",
    "\n",
    "\n",
    "Более подробно об этих и других способах оптимизации можно прочитать:\n",
    " - [здесь](http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms) очень хорошее описание различных способов оптимизации, в этом задании мы опираемся на терминологию из данной статьи\n",
    " - статья про [momentum](https://pdfs.semanticscholar.org/97da/c94ffd7a7ac09a4218848300cc7e98569d77.pdf)\n",
    " - оригинальная статья про [adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    " - оригинальная статья про [adadelta](http://arxiv.org/pdf/1212.5701v1.pdf)\n",
    " - википедия про [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) и [adagrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad)\n",
    " - [визуализация](http://imgur.com/a/Hqolp) разных способов оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Реализуйте метод оптимизации *Momentum* **(0.5 балла)** и один из *Adagrad*/*Adadelta* **(1 балл)**. \n",
    "- Сравните с классическим sgd. \n",
    "- Посмотрите как значение гиперпараметра $\\gamma$ влияет на скорость сходимости и качество в методе *Momentum*.\n",
    "- Дало ли преимущество использование адаптивного шага в методе *Adagrad*/*Adadelta*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Momentum:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def momentum_stoch_grad_descent(x, y, h = 0.5, lambda_ = 0.5, w_init = 'random', gamma = 0.9):\n",
    "    N = x.shape[0]\n",
    "    n = x.shape[1]\n",
    "    all_w_iterations = []\n",
    "    w = init_weights(n, w_init)\n",
    "    all_w_iterations.append(w)\n",
    "    Q = count_functional(x, y, w)\n",
    "    nu = np.zeros(n)\n",
    "    w_old = w\n",
    "    Q_old = Q\n",
    "    nu_old = nu\n",
    "    index = np.random.randint(0, N, 1)\n",
    "    eps = L(x[index[0]], y[index[0]], w_old)\n",
    "    nu = -gamma*nu_old + h*nabla_L(x[index[0]], y[index[0]], w_old)\n",
    "    w = w_old + nu\n",
    "    all_w_iterations.append(w)\n",
    "    Q = lambda_*eps + (1-lambda_)*Q_old\n",
    "    iters_count = 1\n",
    "    while np.linalg.norm(w-w_old) > 1e-6 and np.absolute(Q-Q_old) > 1e-6:\n",
    "        w_old = w\n",
    "        Q_old = Q\n",
    "        nu_old = nu\n",
    "        index = np.random.randint(0, N, 1)\n",
    "        eps = L(x[index[0]], y[index[0]], w_old)\n",
    "        nu = -gamma*nu_old + h*nabla_L(x[index[0]], y[index[0]], w_old)\n",
    "        w = w_old + nu\n",
    "        all_w_iterations.append(w)\n",
    "        Q = lambda_*eps + (1-lambda_)*Q_old\n",
    "        iters_count += 1\n",
    "    return w, iters_count, all_w_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations with eta=0.01:  22717\n",
      "Value of functional with this eta:  0.17129529269\n",
      "Number of iterations with eta=0.02:  5233\n",
      "Value of functional with this eta:  0.176716287624\n",
      "Number of iterations with eta=0.05:  12686\n",
      "Value of functional with this eta:  0.173794034658\n",
      "Number of iterations with eta=0.1:  9923\n",
      "Value of functional with this eta:  0.184135883514\n",
      "Number of iterations with eta=0.3:  786\n",
      "Value of functional with this eta:  0.224359788758\n",
      "Number of iterations with eta=0.5:  9974\n",
      "Value of functional with this eta:  0.331722896479\n",
      "Number of iterations with eta=0.75:  3553\n",
      "Value of functional with this eta:  0.398891578264\n"
     ]
    }
   ],
   "source": [
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.01)\n",
    "print('Number of iterations with eta=0.01: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.02)\n",
    "print('Number of iterations with eta=0.02: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.05)\n",
    "print('Number of iterations with eta=0.05: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.1)\n",
    "print('Number of iterations with eta=0.1: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.3)\n",
    "print('Number of iterations with eta=0.3: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.5)\n",
    "print('Number of iterations with eta=0.5: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.75)\n",
    "print('Number of iterations with eta=0.75: ', iters)\n",
    "print('Value of functional with this eta: ', count_functional(x, y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты относительно sgd ненамного лучше, в некоторых случаях количество итераций становится очень большим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations with gamma=0.7:  12566\n",
      "Value of functional with this gamma:  0.174691148394\n",
      "Number of iterations with gamma=0.5:  63526\n",
      "Value of functional with this gamma:  0.173408895014\n",
      "Number of iterations with gamma=0.4:  74358\n",
      "Value of functional with this gamma:  0.17448178767\n",
      "Number of iterations with gamma=0.1:  43098\n",
      "Value of functional with this gamma:  0.173530884898\n"
     ]
    }
   ],
   "source": [
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.02, gamma=0.7)\n",
    "print('Number of iterations with gamma=0.7: ', iters)\n",
    "print('Value of functional with this gamma: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.02, gamma=0.5)\n",
    "print('Number of iterations with gamma=0.5: ', iters)\n",
    "print('Value of functional with this gamma: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.02, gamma=0.4)\n",
    "print('Number of iterations with gamma=0.4: ', iters)\n",
    "print('Value of functional with this gamma: ', count_functional(x, y, w))\n",
    "\n",
    "w, iters, w_s = momentum_stoch_grad_descent(x, y, 0.02, gamma=0.1)\n",
    "print('Number of iterations with gamma=0.1: ', iters)\n",
    "print('Value of functional with this gamma: ', count_functional(x, y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по результатам, хоть они и зависят от случая, с уменьшением гамма число итераций увеличивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многоклассовая классификация\n",
    "\n",
    "Модель логистической регрессии можно обобщить для случая многоклассовой классификации. Метка класса теперь лежит во множестве $\\{1, 2, ..., K\\}$. Параметры модели $w$ в этом случае являются матрицей размерности $K \\times M$, где $M$ − количество признаков. Обучение модели логистической регрессии в многоклассовом случае будет выглядеть следующим образом:\n",
    "\n",
    "$$ -\\dfrac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K [y_i = k] \\log\\Big(\\frac{\\exp(\\langle w_k, x_i \\rangle)}{\\sum_{s=1}^K \\exp(\\langle w_s, x_i \\rangle))}\\Big) + \\dfrac{C}{2}\\lVert w \\rVert^2  \\to \\min_w$$\n",
    "\n",
    "Здесь $w_k$ обозначает $k$-ую строку матрицы $w$.\n",
    "\n",
    "Обучать эту модель также можно с помощью градиентного спуска.\n",
    "\n",
    "Кроме того существует другой, более универсальный способ решать задачу многоклассовой классификации. Для этого нужно обучить несколько бинарных моделей классификации, после чего на основании предсказаний по этим моделям вынести окончательный вердикт о принадлежности объекта одному из $K$ классов. Существует две популярные стратегии использования бинарных классификаторов для задачи многоклассовой классификации:\n",
    " - OvR (One-vs-Rest, One-vs-All) − стратегия, при которой каждый из $K$ классификаторов обучается отделять объекты одного класса от объектов всех остальных классов. В качестве предсказания используется тот класс, классификатор которого предсказал наибольшую вероятность среди всех.\n",
    " - OvO (One-vs-One) − стратегия, при которой каждый из $\\frac{K(K-1)}{2}$ классификаторов учится разделять объекты пары классов, игнорируя объекты всех остальных классов. На этапе предсказания класс обычно выбирается путем голосования по вердиктам каждого из классификаторов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Покажите, что функция потерь для многоклассовой классификации сводится к [функции потерь для бинарной классификации](#log_reg) при $K=2$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ -\\dfrac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^2 [y_i = k] \\log\\Big(\\frac{\\exp(\\langle w_k, x_i \\rangle)}{\\sum_{s=1}^2 \\exp(\\langle w_s, x_i \\rangle))}\\Big) = \\dfrac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^2 [y_i = k] \\log\\Big(\\frac{\\sum_{s=1}^2 \\exp(\\langle w_s, x_i \\rangle))}{\\exp(\\langle w_k, x_i \\rangle)}\\Big) = \\dfrac{1}{N}\\Big(\\sum_{i=1}^N [y_i = 1] \\log\\Big(\\frac{\\sum_{s=1}^2 \\exp(\\langle w_s, x_i \\rangle))}{\\exp(\\langle w_1, x_i \\rangle)}\\Big) + \\sum_{i=1}^N [y_i = 2] \\log\\Big(\\frac{\\sum_{s=1}^2 \\exp(\\langle w_s, x_i \\rangle))}{\\exp(\\langle w_2, x_i \\rangle)}\\Big)\\Big) = \\dfrac{1}{N}\\Big(\\sum_{i=1}^N [y_i = 1] \\log\\Big(1+\\exp(\\langle w_2-w_1, x_i \\rangle)\\Big) + \\sum_{i=1}^N [y_i = 2] \\log\\Big(1+\\exp(\\langle w_1-w_2, x_i \\rangle)\\Big) = [w = w_1-w_2] = \\dfrac{1}{N}\\Big(\\sum_{i=1}^N [y_i = 1] \\log\\Big(1+\\exp(-\\langle w, x_i \\rangle)\\Big) + \\sum_{i=1}^N [y_i = 2] \\log\\Big(1+\\exp(\\langle w, x_i \\rangle)\\Big) = [2 := -1] = \\dfrac{1}{N}\\Big(\\sum_{i=1}^N [y_i = 1] \\log\\Big(1+\\exp(-\\langle w, x_i \\rangle)\\Big) + \\sum_{i=1}^N [y_i = -1] \\log\\Big(1+\\exp(\\langle w, x_i \\rangle)\\Big) = [\\langle \\rangle \\times y_i^2] = \\dfrac{1}{N}\\Big(\\sum_{i=1}^N [y_i = 1] \\log\\Big(1+\\exp(-y_i\\langle w, x_i \\rangle y_i)\\Big) + \\sum_{i=1}^N [y_i = -1] \\log\\Big(1+\\exp(y_i\\langle w, x_i \\rangle y_i)\\Big) = \\sum_{i=1}^N \\log\\Big(1+\\exp(-\\langle w, x_i \\rangle y_i)\\Big)$, что и требовалось показать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** \n",
    "Сгенерируйте несколько выборок точек с 2 признаками и 3 классами (по 100 объектов каждого класса) на которых будете проводить эксперименты. Для этого можно воспользоваться функцией [make_blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) из пакета sklearn.\n",
    "\n",
    "Обучите [логистическую регрессию](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) из пакета sklearn тремя различными способами:\n",
    " - в режиме multinomial, оптимизирующем многоклассовую функцию потерь\n",
    " - в режиме OvR\n",
    " - в режиме OvO\n",
    " \n",
    "Первые два способа реализованы в самом классе LogisticRegression, в то время как для решения задачи третьим методом в sklearn реализован класс [OneVsOneClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html) (класс для OvR схемы [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html), конечно, также присутствует в пакете).\n",
    " \n",
    " \n",
    " Проделайте следующие шаги для каждой стратегии и прокомментируйте полученные результаты:\n",
    " - Изобразите точки выборки, а также разделяющие прямые (их должно быть по 3 для каждой из стратегий). Проведите эксперимент на всех сгенерированных выборках.\n",
    " - Какие особенности, преимущества и недостатки с точки зрения построения разделяющих плоскостей, качества разделения классов и вычислительной эффективности характерны для каждого метода?\n",
    " - Для каждой из стратегий приведите примеры ситуаций, когда стоит выбирать ее для решения задачи многоклассовой классификации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия\n",
    "\n",
    "![](http://66.147.244.197/~globerov/introspectivemode/wp-content/uploads/2012/08/regression-265x300.jpeg)\n",
    "\n",
    "Метод градиентного спуска позволяет оптимизировать произвольные функции. Например, рассмотрим задачу линейной регрессии, где $y \\in \\mathbb{R}$, а алгоритм будет иметь вид $a(x) = \\langle w, x\\rangle$. В случае метода наименьших квадратов оптимизируемый функционал можно записать следующим образом:\n",
    "\n",
    "$$ \\sum_{i=1}^N (\\langle w, x_i \\rangle - y_i) ^ 2 \\to \\min_w$$\n",
    "\n",
    "Эта задача интересна тем, что для нее можно выписать аналитическое решение. Попробуем сравнить эти подходы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте выборку из 600 точек с двумя признаками для задачи регрессии, воспользовавшись функцией [make_regression](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Выпишите как выглядит точное решение задачи линейной регрессии. Решите задачу регрессии с помощью этого подхода без использования и с использованием регуляризации. Есть ли недостатки у такого подхода к решению задачи?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Примените метод стохастического градиентного спуска реализованный ранее. Сильно ли отличается полученный вектор параметров по сравнению с точным решением? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, линейная регрессия позволяет хорошо восстанавливать *линейные* зависимости, однако в общем случае хуже работает с более сложными данными. Это хорошо можно увидеть на следующем примере.\n",
    "\n",
    "Пусть исходная зависимость имеет вид $y = x \\cdot sin(x)$. Сгенерируем соответствующие точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, 100)\n",
    "y = X * np.sin(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKHS8eiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f568eab0ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X_train, y_train)\n",
    "_ = plt.scatter(X_test, y_test, c='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если теперь к полученным данным применить модель линейной регрессии, то получим следующее решение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train[:, np.newaxis], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAEACAYAAAAA4lT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIg4SsmFiIiIOErJhYiIiDjq/wPaLPPZyvXhKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f568e77aa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X_train, y_train)\n",
    "_ = plt.scatter(X_test, y_test, c='g')\n",
    "y_plot = lr.predict(X[:, np.newaxis])\n",
    "_ = plt.plot(X, y_plot, c='r')\n",
    "_ = plt.legend(('linear regression', 'train', 'test'), bbox_to_anchor=(1.05, 1), loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество модели регрессии можно оценивать с помощью некоторых метрик, например $MSE = \\sum_{i=1}^l(a(x_i) - y_i)^2$ и в данном случае оно равно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.759977502823233"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr.predict(X_test[:, np.newaxis])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построенное решение совсем отдаленно напоминает исходную зависимость. Одним из способов улучшить результат является добавление всех попарных произведений признаков, а также степеней: $x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "**(0.5 балла)** Воспользуйтесь классом [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) и добавьте к данным зависимости $y = x \\cdot sin(x)$ различные полиномы.\n",
    "\n",
    " - рассмотрите как степень полинома (от 1 до 20) влияет на качество\n",
    " - изобразите на графике предсказание аналогично линейной регрессии\n",
    " - сравните этот подход с функцией [polyfit](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html)\n",
    "\n",
    "В чем могут быть недостатки такого подхода?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный SVM\n",
    "\n",
    "Вернемся к задаче бинарной классификации. Будем обозначать обучающую выборку $\\{(x_n, y_n)\\}_{n=1}^N$, где $N$ — количество объектов, $\\boldsymbol x_n \\in \\mathbb{R}^d$ — числовой вектор признакового описания объекта, $y_n \\in \\{+1, -1\\}$ — класс объекта.\n",
    "\n",
    "SVM обучает модель разделяющей гиперплоскости:\n",
    "$$f(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + b$$\n",
    "Параметры модели — вектор весов $\\boldsymbol w \\in \\mathbb{R}^d$ и сдвиг $b \\in \\mathbb{R}$.\n",
    "\n",
    "Обучение модели происходит путем решения оптимизационной задачи:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{n=1}^N \\xi_n \\to \\min_{\\boldsymbol w, \\boldsymbol \\xi, b} \\\\\n",
    "    \\text{s.t.: } \\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1 - \\xi_n, \\quad \\xi_n \\geq 0, \\quad \\forall n=1,\\dots,N\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Ограничения вида $y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1$ требуют, чтобы объекты правильно классифицировались разделяющей гиперплоскостью. Поскольку линейная разделимость выборки не гарантируется на практике, вводят переменные $\\xi_n$ (slack variables), которые ослабляют ограничения правильной классификации. В оптимизируемом функционале слагаемое $\\| \\boldsymbol w \\|^2$ штрафует малую ширину разделяющей гиперплоскости, сумма $\\sum_n \\xi_n$ штрафует ослабление ограничений. \n",
    "\n",
    "После нахождения решения оптимизационной задачи $(\\boldsymbol w_{\\star}, \\boldsymbol \\xi_{\\star}, b_{\\star})$, часть ограничений становятся _активными_, т.е. переходят в \"крайнее положение\" — точное равенство:\n",
    "$$\\quad y_n (\\boldsymbol w_{\\star}^T \\boldsymbol x_n + b_{\\star}) = 1 - \\xi_{\\star,n}$$\n",
    "Объекты, соответствующие активным ограничениям, называются _опорными_.\n",
    "\n",
    "Гиперпараметр $C$ задает баланс между шириной разделяющей полосы и ошибками, допускаемыми классификатором. Обратите внимание, что $C$ фиксируется до обучения и не оптимизируется вместе с параметрами модели. Этот гиперпараметр отвечает за обобщающую способность разделяющей гиперплоскости, высокая обобщающая способность (соответствующая большому значению $C$) может привести к переобучению, если линейная модель хорошо описывает обучающие примеры. При подборе оптимального гиперпараметра $C$ необходимо оценивать качество на отложенной выборке или кросс-валидации. Как правило, для конкретной задачи заранее неизвестно, какой порядок имеет оптимальное значение гиперпараметра $C$, поэтому перебирать значения лучше по логарифмической сетке, например: $10^{-3}, 10^{-2}, \\dots, 10^{5}$.\n",
    "\n",
    "Особенность этого метода в том, что он имеет решение, которое может быть найдено используя квадратичное программирование. В этом задании мы не будем сводить данную задачу к задаче квадратичного программирования, а воспользуемся готовой реализацией из библиотеки sklearn.\n",
    "\n",
    "### Особенности реализации\n",
    "\n",
    "Обратите внимание, что в библиотеке sklearn можно найти 2 реализации линейного SVM: [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) и [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) с ядровой функцией *linear*. Эти реализации имеют различие в библиотеках, на которых основаны: в первом случае используется библиотека *liblinear*, во втором — *libsvm*. Каждая из библиотек имеет свои плюсы, поэтому перед применением стоит определиться какая из реализаций подходит больше. Обратите внимание, что это различие есть только для линейного SVM.\n",
    "\n",
    "В данном задании рекомендуем использовать класс [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) с параметром *kernel='linear'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте три случайные двумерные выборки для бинарной классификации (хотя бы по 400 точек в каждой):\n",
    "- с линейно-разделимыми классами\n",
    "- с хорошо разделимыми классами, но не линейно\n",
    "- с плохо разделимыми классами по имеющимся признакам\n",
    "    \n",
    "Для генерации случайной выборки можно воспользоваться функциями, которые находятся в пакете [sklearn.datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). Для того чтобы выборки не менялись при перезапуске ноутбука, фиксируйте параметр *random_state*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** Протестируйте линейный SVM  на сгенерированных выборках. Покажите на плоскости разделяющую прямую и линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Проделайте это для разных значений параметра $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** Как зависит число опорных векторов от параметра $C$ для различных выборок?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Явное преобразование признаков\n",
    "\n",
    "Как и в случае с линейной регрессией, когда оптимальная разделяющая гиперплоскость не является линейной, данная модель является очень грубым решением. Линейная неразделимость объектов может быть исправлена путем перехода в другое признаковое пространство, в котором линейная модель лучше описывает данные и, возможно, существует правильно классифицирующая разделяющая гиперплоскость:\n",
    "\n",
    "$$\\boldsymbol x \\in \\mathbb{R}^d \\mapsto \\phi(\\boldsymbol x) \\in \\mathbb{R}^t$$\n",
    "\n",
    "Так, например, аналогичное добавление всех попарных произведений признаков: $\\phi(x_1, \\dots, x_d) = (x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2)$ переводит в пространство, в котором линейная гиперплоскость является квадратичной формой в исходном пространстве и в исходном пространстве признаков разделяющая поверхность может быть, скажем, эллипсом.\n",
    "\n",
    "[Видеоролик с демонстрацией](https://youtu.be/9NrALgHFwTo)\n",
    "\n",
    "### Задание\n",
    "\n",
    "**(1 балл)** На тех же данных используя явное преобразование признаков обучите методом опорных векторов квадратичную разделяющую поверхность. Покажите на плоскости разделяющую прямую и линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Проделайте это для разных значений параметра $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двойственный переход и Ядровой SVM\n",
    "\n",
    "![](http://i.imgur.com/bJAzRCt.png)\n",
    "\n",
    "Задачу обучения линейного SVM, рассмотренную в предыдущем пункте принято называть _прямой_ оптимизационной задачей для SVM. Любая задача оптимизации с ограничениями имеет [_двойственную_ задачу Лагранжа](http://goo.gl/OujTPr), в которой оптимизируются _двойственные переменные_ (множители Лагранжа), соответствующие штрафу за нарушение ограничений, максимизируется нижняя оценка функционала прямой задачи. В случае задачи квадратичного программирования, решение двойственной задачи (значение оптимизируемого функционала) совпадает с оптимумом прямой задачи.\n",
    "\n",
    "Подробнее можно почитать в [статье](http://www.machinelearning.ru/wiki/images/2/25/SMAIS11_SVM.pdf).\n",
    "\n",
    "Двойственная задача для SVM имеет вид:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\sum_{n} \\alpha_n - \\frac{1}{2}\\sum_{n}\\sum_{n'} \\alpha_{n}\\alpha_{n'} y_{n}y_{n'} x_{n}^Tx_{n'} \\to \\max_{\\alpha} \\\\\n",
    "    \\begin{aligned}\n",
    "        \\text{s.t. } \\quad  \n",
    "        & 0 \\le \\alpha_n \\le C, \\quad \\forall n = 1, \\dots, N \\\\\n",
    "        & \\sum_{n} \\alpha_n y_n = 0\n",
    "    \\end{aligned}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Оптимизируется вектор из двойственных переменных $\\alpha_n$, соответствующих объектам обучающей выборки. Объект $x_n$ является опорным, если $\\alpha_n > 0$.\n",
    "\n",
    "Предсказание вычисляется по следующему правилу:\n",
    "$$\\hat{y}(x) = \\text{sign}\\left(\\sum_{n}\\alpha_{n}y_{n}x^Tx_{n} + b\\right).$$\n",
    "\n",
    "Для предсказания необходимо оценить значение $b$. Известно, что для любого опорного объекта, который классифицируется безошибочно верно:\n",
    "$$y_n = \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'} + b,$$\n",
    "значит для любого такого объекта:\n",
    "$$b = y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}.$$\n",
    "\n",
    "В случае наличия ошибок классификации обучающей выборки, предлагается усреднять значение $b$ по всем опорным векторам:\n",
    "$$b = \\frac{1}{N_\\text{SV}}\\sum_{n \\in \\text{SV}}\\left(y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}\\right).$$\n",
    "Интуиция здесь такова, что суммарные ошибки в положительную сторону примерно равны суммарным ошибкам в отрицательную сторону.\n",
    "\n",
    "Другой вариант — отказаться от параметра $b$ и работать с моделью $f(x) = w^Tx$, добавив к вектору $x$ константный признак.\n",
    "\n",
    "#### Неявное преобразование признаков\n",
    "Отметим, что двойственная задача SVM содержит вектора признаков исключительно в виде скалярного произведения $x^Tx'$. Эта особенность позволяет производить неявное преобразование признакового пространства. Вместо вычисления функции $\\phi(\\boldsymbol x)$, которая может отображать исходные признаки в вектора очень большой размерности, будем вычислять скалярное произведение $k(\\boldsymbol x, \\boldsymbol x') = \\phi(\\boldsymbol x)^T\\phi(\\boldsymbol x')$ называемое _ядром_. \n",
    "\n",
    "\n",
    "В этом задании используйте класс $sklearn.svm.SVC$, меняя тип ядра. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "**(1.5 балла)** Протестируйте на предыдущих двумерных выборках ядровой SVM. Покажите на плоскости строящиеся разделяющие поверхности, линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Попробуйте следующие ядровые функции:\n",
    "- линейная: $k(x, x') = x^Tx'$\n",
    "- полиномиальная: $k(x, x') = (x^Tx' + 1)^d$ с различными степенями $d = 2,3,\\dots$\n",
    "- Гауссовская-RBF: $k(x, x') = \\exp(-\\sigma\\|x - x'\\|^2)$\n",
    "\n",
    "Ответьте на следующие вопросы:\n",
    " - Как ведет себя SVM с полиномиальным ядром в зависимости от параметров $C$ и степени ядра $d$?\n",
    " - Как ведет себя SVM с RBF-ядром в зависимости от параметров $C$ и $\\sigma$? Поварьируйте параметры $C$ и $\\sigma$ по логарифмической сетке. Какие значения параметров ведут к переобучению, а какие — к слишком грубой модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "**(1 балл)** В этой работе вы рассмотрели некоторые линейные модели машинного обучения, а также способы их обучения. Ответьте на следующие вопросы:\n",
    "\n",
    " - Какие есть достоинства у рассмотренных моделей? Поясните свой ответ для каждой модели.\n",
    " - Каким общим недостатком обладают данные модели? Какие есть способы его устранения? В чем может заключаться сложность использования этого подхода?\n",
    " - В чем заключаются различия с точки зрения обучения алгоритмов? Какие есть достоинства и недостатки у рассмотренных методов обучения?\n",
    " - Предположите в каком случае каждый из алгоритмов будет работать лучше: при большом/небольшом количестве данных? Поясните почему. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Достоинства gd:**\n",
    "- быстр относительно методов второго порядка.\n",
    "- прост в реализации.\n",
    "- детерминирован, что упрощает проверку работы.\n",
    "\n",
    "**Достоинства sgd:**\n",
    "- быстр относительно gd.\n",
    "- для его разработано несколько улучшений.\n",
    "- прост в реализации.\n",
    "- подходит для задач с большими данными\n",
    "- рандомизирован, и его можно запускать несколько раз в надежде получить лучший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Недостатки:**\n",
    "- сильно переобучаются.\n",
    "- можут не сойтись или разойтись\n",
    "- застревание в локальных экстремумах\n",
    "\n",
    "Способы: регуляризация, изменение градиентного шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgd удобно применять, если данных много потому что gd будет очень долго в этом случае выполнять одну итерацию, а за это время sgd выполнит их много-много и, скорее всего, эти неколько итераций в сумме лучше оптимизируют функционал, чем одна долгая итерация. Если данных мало, то итерация gd относительно итерации sgd будет выполняться не так долго, а, следовательно, потеря во времени небольшая, зато в оптимизации можно преуспеть."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
