{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/35\n",
      "45000/45000 [==============================] - 127s 3ms/step - loss: 1.9939 - acc: 0.2692 - val_loss: 1.6570 - val_acc: 0.4068\n",
      "Epoch 2/35\n",
      "45000/45000 [==============================] - 207s 5ms/step - loss: 1.5809 - acc: 0.4253 - val_loss: 1.3620 - val_acc: 0.5120\n",
      "Epoch 3/35\n",
      "45000/45000 [==============================] - 138s 3ms/step - loss: 1.3653 - acc: 0.5062 - val_loss: 1.2813 - val_acc: 0.5456\n",
      "Epoch 4/35\n",
      "37120/45000 [=======================>......] - ETA: 38s - loss: 1.2370 - acc: 0.5558"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4b6fd714bdc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m               verbose=1)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Оцениваем качество обучения модели на тестовых данных\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 12 11:58:58 2019\n",
    "\n",
    "@author: asus\n",
    "\"\"\"\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Задаем seed для повторяемости результатов\n",
    "numpy.random.seed(42)\n",
    "\n",
    "# Загружаем данные\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# Размер мини-выборки\n",
    "batch_size = 32\n",
    "# Количество классов изображений\n",
    "nb_classes = 10\n",
    "# Количество эпох для обучения\n",
    "nb_epoch = 35\n",
    "\n",
    "# Нормализуем данные\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Преобразуем метки в категории\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# Создаем последовательную модель\n",
    "model = Sequential()\n",
    "# Формирование вектора, отвечающего за размерность входных данных: \n",
    "# либо (кол-во каналов, ширина, высота), либо (ширина, высота, кол-во каналов) \n",
    "# (в зависимости от значения параметра \"image_data_format\" в файле keras.json)\n",
    "shape_vector = X_train.shape[1:]\n",
    "# Первый сверточный слой\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                        input_shape=shape_vector, activation='relu'))\n",
    "# Второй сверточный слой\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# Первый слой подвыборки\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Слой регуляризации Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Третий сверточный слой\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "# Четвертый сверточный слой\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Второй слой подвыборки\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Слой регуляризации Dropout\n",
    "model.add(Dropout(0.25))\n",
    "# Слой преобразования данных из 2D представления в плоское\n",
    "model.add(Flatten())\n",
    "# Полносвязный слой для классификации\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# Слой регуляризации Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Выходной полносвязный слой\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# Задаем параметры оптимизации\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "# Обучаем модель\n",
    "model.fit(X_train, Y_train,\n",
    "              batch_size=128,\n",
    "              epochs=nb_epoch,\n",
    "              validation_split=0.1,\n",
    "              shuffle=True,\n",
    "              verbose=1)\n",
    "\n",
    "# Оцениваем качество обучения модели на тестовых данных\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Точность работы на тестовых данных: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# model_json = model.to_json()\n",
    "# # Записываем модель в файл\n",
    "# json_file = open(\"cifar_nn.json\", \"w\")\n",
    "# json_file.write(model_json)\n",
    "# json_file.close()\n",
    "# model.save_weights(\"cifar_nn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_filename = \"cifar_nn.json\"\n",
    "with open(json_filename, \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Загружаем веса в модель\n",
    "h5_filename = \"cifar_nn.h5\"\n",
    "model.load_weights(h5_filename)\n",
    "\n",
    "# Перед использованием загруженной нейронной сети необходимо её скомпилировать\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", \n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.io import imshow \n",
    "%matplotlib inline\n",
    "\n",
    "index = 11  \n",
    "\n",
    "imshow(X_test[index])\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "res = model.predict(X_test[index].reshape((1, 32, 32, 3)))\n",
    "print(np.argmax(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 75)        1950      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 75)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 75)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 100)         187600    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 100)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               800500    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 995,060\n",
      "Trainable params: 995,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/20\n",
      "33600/33600 [==============================] - 50s 1ms/step - loss: 0.3318 - acc: 0.8957 - val_loss: 0.0718 - val_acc: 0.9779\n",
      "Epoch 2/20\n",
      "33600/33600 [==============================] - 47s 1ms/step - loss: 0.0784 - acc: 0.9756 - val_loss: 0.0482 - val_acc: 0.9857\n",
      "Epoch 3/20\n",
      "33600/33600 [==============================] - 44s 1ms/step - loss: 0.0518 - acc: 0.9834 - val_loss: 0.0338 - val_acc: 0.9892\n",
      "Epoch 4/20\n",
      "33600/33600 [==============================] - 43s 1ms/step - loss: 0.0386 - acc: 0.9871 - val_loss: 0.0314 - val_acc: 0.9907\n",
      "Epoch 5/20\n",
      "33600/33600 [==============================] - 44s 1ms/step - loss: 0.0323 - acc: 0.9896 - val_loss: 0.0344 - val_acc: 0.9893\n",
      "Epoch 6/20\n",
      "33600/33600 [==============================] - 53s 2ms/step - loss: 0.0269 - acc: 0.9913 - val_loss: 0.0286 - val_acc: 0.9907\n",
      "Epoch 7/20\n",
      "33600/33600 [==============================] - 58s 2ms/step - loss: 0.0251 - acc: 0.9919 - val_loss: 0.0343 - val_acc: 0.9906\n",
      "Epoch 8/20\n",
      "33600/33600 [==============================] - 51s 2ms/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0276 - val_acc: 0.9919\n",
      "Epoch 9/20\n",
      "33600/33600 [==============================] - 83s 2ms/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0277 - val_acc: 0.9929\n",
      "Epoch 10/20\n",
      "33600/33600 [==============================] - 97s 3ms/step - loss: 0.0170 - acc: 0.9945 - val_loss: 0.0281 - val_acc: 0.9926\n",
      "Epoch 11/20\n",
      "33600/33600 [==============================] - 100s 3ms/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.0261 - val_acc: 0.9932\n",
      "Epoch 12/20\n",
      "33600/33600 [==============================] - 94s 3ms/step - loss: 0.0136 - acc: 0.9952 - val_loss: 0.0281 - val_acc: 0.9929\n",
      "Epoch 13/20\n",
      "33600/33600 [==============================] - 90s 3ms/step - loss: 0.0142 - acc: 0.9951 - val_loss: 0.0300 - val_acc: 0.9924\n",
      "Epoch 14/20\n",
      "33600/33600 [==============================] - 111s 3ms/step - loss: 0.0134 - acc: 0.9958 - val_loss: 0.0360 - val_acc: 0.9917\n",
      "Epoch 15/20\n",
      "33600/33600 [==============================] - 68s 2ms/step - loss: 0.0129 - acc: 0.9960 - val_loss: 0.0293 - val_acc: 0.9932\n",
      "Epoch 16/20\n",
      "33600/33600 [==============================] - 74s 2ms/step - loss: 0.0112 - acc: 0.9966 - val_loss: 0.0317 - val_acc: 0.9931\n",
      "Epoch 17/20\n",
      "33600/33600 [==============================] - 62s 2ms/step - loss: 0.0102 - acc: 0.9968 - val_loss: 0.0342 - val_acc: 0.9911\n",
      "Epoch 18/20\n",
      "33600/33600 [==============================] - 44s 1ms/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0304 - val_acc: 0.9926\n",
      "Epoch 19/20\n",
      "33600/33600 [==============================] - 49s 1ms/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0385 - val_acc: 0.9906\n",
      "Epoch 20/20\n",
      "33600/33600 [==============================] - 60s 2ms/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0366 - val_acc: 0.9918\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 12 15:09:27 2019\n",
    "\n",
    "@author: asus\n",
    "\"\"\"\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.backend import image_data_format\n",
    "\n",
    "# Устанавливаем seed для повторяемости результатов\n",
    "# numpy.random.seed(42)\n",
    "\n",
    "# Загружаем данные\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# print(X_train.shape)\n",
    "\n",
    "# # Формирование вектора размерности (зависит от параметра из файла keras.json)\n",
    "\n",
    "# # Преобразование размерности изображений\n",
    "# X_train = X_train.reshape(X_train.shape[0], *input_shape)\n",
    "# X_test = X_test.reshape(X_test.shape[0], *input_shape)\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('digit-recognizer/train.csv')\n",
    "test = pd.read_csv('digit-recognizer/test.csv')\n",
    "sample = pd.read_csv('digit-recognizer/sample_submission.csv')\n",
    "X_train = train.values[:, 1:].reshape((train.shape[0], 28, 28))\n",
    "y_train = train.values[:, 0]\n",
    "X_submission = test.values.reshape((test.shape[0], 28, 28))\n",
    "\n",
    "input_shape = ((1, *X_train.shape[1:]) if \n",
    "               image_data_format() == 'channels_first' else \n",
    "               (*X_train.shape[1:], 1))\n",
    "X_train = X_train.reshape(X_train.shape[0], *input_shape)\n",
    "X_submission = X_submission.reshape(X_submission.shape[0], *input_shape)\n",
    "\n",
    "# Нормализация данных\n",
    "X_train = X_train.astype('float32')\n",
    "X_submission = X_submission.astype('float32')\n",
    "X_train /= 255\n",
    "X_submission /= 255\n",
    "\n",
    "# Преобразуем метки в категории\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "# Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Создаем последовательную модель\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(75, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(100, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Компилируем модель\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Обучаем сеть\n",
    "model.fit(X_train, Y_train, batch_size=200, epochs=20, validation_split=0.2, \n",
    "          verbose=1)\n",
    "\n",
    "# Оцениваем качество обучения сети на тестовых данных\n",
    "# scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "# print(\"Точность работы на тестовых данных: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "model_json = model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"mnist_nn.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "model.save_weights(\"mnist_nn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission = model.predict(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.argmax(y_submission, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumb = pd.DataFrame(data = [[i+1, res[i]] for i in range(res.shape[0])], columns=['ImageID', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumb.to_csv('kk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
